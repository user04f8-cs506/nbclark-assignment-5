{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean', weighted=False):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.weighted = weighted\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = self.compute_distance(x, self.X_train)\n",
    "            indices = np.argsort(distances)[:self.k]\n",
    "            k_labels = self.y_train[indices]\n",
    "            if self.weighted:\n",
    "                k_distances = distances[indices]\n",
    "                k_distances = np.where(k_distances == 0, 1e-5, k_distances)\n",
    "                weights = 1 / k_distances\n",
    "                predicted_prob = np.average(k_labels, weights=weights)\n",
    "            else:\n",
    "                predicted_prob = np.mean(k_labels)\n",
    "            predictions.append(predicted_prob)\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def compute_distance(self, x, X_train):\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            distances = np.sqrt(np.sum((X_train - x) ** 2, axis=1))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            distances = np.sum(np.abs(X_train - x), axis=1)\n",
    "        else:\n",
    "            raise ValueError('Unsupported distance metric')\n",
    "        return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_path, test_path):\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    test_data = pd.read_csv(test_path)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    train_data = train_data.drop(['CustomerId', 'Surname'], axis=1)\n",
    "    test_data = test_data.drop(['CustomerId', 'Surname'], axis=1)\n",
    "\n",
    "    # Store 'id' column from test_data\n",
    "    test_ids = test_data['id']\n",
    "\n",
    "    # Drop 'id' column from train and test data\n",
    "    train_data = train_data.drop(['id'], axis=1)\n",
    "    test_data = test_data.drop(['id'], axis=1)\n",
    "\n",
    "    # Map 'Gender' to numerical values\n",
    "    gender_mapping = {'Male': 0, 'Female': 1}\n",
    "    train_data['Gender'] = train_data['Gender'].map(gender_mapping)\n",
    "    test_data['Gender'] = test_data['Gender'].map(gender_mapping)\n",
    "\n",
    "    # One-hot encode 'Geography'\n",
    "    train_data = pd.get_dummies(train_data, columns=['Geography'], drop_first=True)\n",
    "    test_data = pd.get_dummies(test_data, columns=['Geography'], drop_first=True)\n",
    "\n",
    "    # Align test data columns with train data\n",
    "    missing_cols = set(train_data.columns) - set(test_data.columns) - set(['Exited'])\n",
    "    for c in missing_cols:\n",
    "        test_data[c] = 0\n",
    "    test_data = test_data[train_data.drop('Exited', axis=1).columns]\n",
    "\n",
    "    # Separate features and target\n",
    "    X_train = train_data.drop('Exited', axis=1)\n",
    "    y_train = train_data['Exited']\n",
    "\n",
    "    # Combine features for scaling\n",
    "    features = pd.concat([X_train, test_data], axis=0)\n",
    "\n",
    "    # Convert all data to float\n",
    "    features = features.astype(float)\n",
    "\n",
    "    # Min-Max Scaling\n",
    "    feature_min = features.min()\n",
    "    feature_max = features.max()\n",
    "    features = (features - feature_min) / (feature_max - feature_min)\n",
    "\n",
    "    # Split back into train and test data\n",
    "    X_train = features.iloc[:len(X_train), :]\n",
    "    X_test = features.iloc[len(X_train):, :]\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X_train = X_train.values\n",
    "    y_train = y_train.values\n",
    "    X_test = X_test.values\n",
    "\n",
    "    return X_train, y_train, X_test, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_score(y_true, y_scores):\n",
    "    # Ensure numpy arrays\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_scores = np.asarray(y_scores)\n",
    "\n",
    "    # Get positive and negative scores\n",
    "    pos = y_scores[y_true == 1]\n",
    "    neg = y_scores[y_true == 0]\n",
    "\n",
    "    # Total number of positive and negative samples\n",
    "    n_pos = len(pos)\n",
    "    n_neg = len(neg)\n",
    "\n",
    "    # Handle cases with no positive or negative samples\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate ranks\n",
    "    combined_scores = np.concatenate([pos, neg])\n",
    "    combined_labels = np.concatenate([np.ones(n_pos), np.zeros(n_neg)])\n",
    "    sorted_indices = np.argsort(combined_scores)\n",
    "    sorted_labels = combined_labels[sorted_indices]\n",
    "\n",
    "    # Calculate rank sums for positive samples\n",
    "    rank = np.arange(1, n_pos + n_neg + 1)\n",
    "    rank_sum = np.sum(rank[sorted_labels == 1])\n",
    "\n",
    "    # Compute AUC using Mann-Whitney U statistic\n",
    "    auc = (rank_sum - n_pos * (n_pos + 1) / 2) / (n_pos * n_neg)\n",
    "    return auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(X, y, test_size=0.2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    train_indices = indices[:split_idx]\n",
    "    val_indices = indices[split_idx:]\n",
    "    X_train = X[train_indices]\n",
    "    X_val = X[val_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_val = y[val_indices]\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_search(X, y, param_distributions, n_iter=10):\n",
    "    best_auc = 0\n",
    "    best_params = None\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train_rs, X_val_rs, y_train_rs, y_val_rs = train_validation_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        # Randomly sample hyperparameters\n",
    "        params = {key: random.choice(values) for key, values in param_distributions.items()}\n",
    "\n",
    "        # Create and train the model\n",
    "        knn = KNN(k=params['k'], distance_metric=params['distance_metric'], weighted=params['weighted'])\n",
    "        knn.fit(X_train_rs, y_train_rs)\n",
    "\n",
    "        # Predict on validation set\n",
    "        y_scores = knn.predict(X_val_rs)\n",
    "\n",
    "        # Compute ROC AUC\n",
    "        auc = roc_auc_score(y_val_rs, y_scores)\n",
    "        print(f\"Params: {params}, AUC: {auc}\")\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_params = params\n",
    "\n",
    "    print(f\"\\nBest AUC: {best_auc} with params: {best_params}\")\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_k_fold(X, y, n_splits):\n",
    "    from collections import defaultdict\n",
    "    # Group indices by class\n",
    "    indices_by_class = defaultdict(list)\n",
    "    for idx, label in enumerate(y):\n",
    "        indices_by_class[label].append(idx)\n",
    "\n",
    "    folds = [[] for _ in range(n_splits)]\n",
    "    # Distribute indices to folds\n",
    "    for label, indices in indices_by_class.items():\n",
    "        np.random.shuffle(indices)\n",
    "        for i, idx in enumerate(indices):\n",
    "            folds[i % n_splits].append(idx)\n",
    "    return folds\n",
    "\n",
    "def hyperparameter_tuning(X, y, param_grid, max_evals=10):\n",
    "    from itertools import product\n",
    "\n",
    "    # Generate all combinations of hyperparameters\n",
    "    all_params = list(product(*param_grid.values()))\n",
    "    np.random.shuffle(all_params)  # Shuffle for randomness\n",
    "\n",
    "    best_auc = 0\n",
    "    best_params = None\n",
    "    evaluations = 0\n",
    "\n",
    "    for params in all_params:\n",
    "        if evaluations >= max_evals:\n",
    "            break\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        auc = cross_validate(X, y, **param_dict)\n",
    "        print(f\"Params: {param_dict}, AUC: {auc}\")\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_params = param_dict\n",
    "        evaluations += 1\n",
    "\n",
    "    print(f\"\\nBest AUC: {best_auc} with params: {best_params}\")\n",
    "    return best_params\n",
    "\n",
    "def cross_validate(X, y, k, distance_metric, weighted, n_splits=5):\n",
    "    np.random.seed(42)\n",
    "    folds = stratified_k_fold(X, y, n_splits)\n",
    "    auc_scores = []\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        val_indices = folds[i]\n",
    "        train_indices = [idx for fold in folds[:i] + folds[i+1:] for idx in fold]\n",
    "\n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_val, y_val = X[val_indices], y[val_indices]\n",
    "\n",
    "        # Fit the model\n",
    "        knn = KNN(k=k, distance_metric=distance_metric, weighted=weighted)\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        # Predict probabilities on validation set\n",
    "        y_scores = knn.predict(X_val)\n",
    "\n",
    "        # Compute ROC AUC\n",
    "        auc = roc_auc_score(y_val, y_scores)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    return mean_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset of the data for hyperparameter tuning\n",
    "def sample_data(X, y, sample_size=1000):\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.choice(len(X), size=sample_size, replace=False)\n",
    "    return X[indices], y[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'k': 5, 'distance_metric': 'euclidean', 'weighted': False}, AUC: 0.8978633351404054\n",
      "Params: {'k': 9, 'distance_metric': 'euclidean', 'weighted': True}, AUC: 0.9085972077004341\n",
      "Params: {'k': 5, 'distance_metric': 'euclidean', 'weighted': False}, AUC: 0.8978633351404054\n",
      "Params: {'k': 3, 'distance_metric': 'euclidean', 'weighted': True}, AUC: 0.8954994405766166\n",
      "Params: {'k': 9, 'distance_metric': 'euclidean', 'weighted': True}, AUC: 0.9085972077004341\n",
      "Params: {'k': 19, 'distance_metric': 'euclidean', 'weighted': False}, AUC: 0.9143189995650814\n",
      "Params: {'k': 9, 'distance_metric': 'manhattan', 'weighted': False}, AUC: 0.9106639191933448\n",
      "Params: {'k': 3, 'distance_metric': 'euclidean', 'weighted': False}, AUC: 0.8872163106010901\n",
      "Params: {'k': 13, 'distance_metric': 'manhattan', 'weighted': True}, AUC: 0.9143719225777035\n",
      "Params: {'k': 9, 'distance_metric': 'manhattan', 'weighted': True}, AUC: 0.9154358108314409\n",
      "Params: {'k': 5, 'distance_metric': 'manhattan', 'weighted': True}, AUC: 0.9072015328675656\n",
      "Params: {'k': 13, 'distance_metric': 'manhattan', 'weighted': False}, AUC: 0.9139091854673407\n",
      "Params: {'k': 3, 'distance_metric': 'manhattan', 'weighted': True}, AUC: 0.9028421693278574\n",
      "Params: {'k': 15, 'distance_metric': 'euclidean', 'weighted': False}, AUC: 0.9138956154641042\n",
      "Params: {'k': 13, 'distance_metric': 'euclidean', 'weighted': True}, AUC: 0.9121762960540466\n",
      "Params: {'k': 3, 'distance_metric': 'euclidean', 'weighted': False}, AUC: 0.8872163106010901\n",
      "Params: {'k': 5, 'distance_metric': 'euclidean', 'weighted': True}, AUC: 0.9063018416529892\n",
      "Params: {'k': 15, 'distance_metric': 'manhattan', 'weighted': False}, AUC: 0.9142701475534302\n",
      "Params: {'k': 13, 'distance_metric': 'euclidean', 'weighted': False}, AUC: 0.9115439339032282\n",
      "Params: {'k': 13, 'distance_metric': 'euclidean', 'weighted': False}, AUC: 0.9115439339032282\n",
      "\n",
      "Best AUC: 0.9154358108314409 with params: {'k': 9, 'distance_metric': 'manhattan', 'weighted': True}\n"
     ]
    }
   ],
   "source": [
    "X, y, X_test, test_ids = preprocess_data('train.csv', 'test.csv')\n",
    "\n",
    "# Define hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'k': list(range(3, 20, 2)),  # Odd values between 3 and 19\n",
    "    'distance_metric': ['euclidean', 'manhattan'],\n",
    "    'weighted': [True, False]\n",
    "}\n",
    "\n",
    "# Perform random search\n",
    "best_params = random_search(X, y, param_distributions, n_iter=20)\n",
    "\n",
    "# Train on full dataset with optimal hyperparameters\n",
    "knn = KNN(k=best_params['k'], distance_metric=best_params['distance_metric'], weighted=best_params['weighted'])\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = knn.predict(X_test)\n",
    "\n",
    "# Save test predictions\n",
    "submission = pd.DataFrame({'id': test_ids, 'Exited': test_predictions})\n",
    "submission.to_csv('submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Columns:\n",
      "['id', 'CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited']\n",
      "\n",
      "Test Data Columns:\n",
      "['id', 'CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load training and test data separately\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Print column names\n",
    "print(\"Training Data Columns:\")\n",
    "print(train_data.columns.tolist())\n",
    "\n",
    "print(\"\\nTest Data Columns:\")\n",
    "print(test_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
